# Activation function: sigmoid, relu, softmax
# apply a non linear transformation and decide wheter a neuron should be activated or not

# if we don't use a activation function, we get a linear output
# our network is just a stacked linear regression model
# not suited for more complex tasks

# with non linear transformation out network can learn better and perform more complex tasks
# after each layer we typically use activation function!

# Activation functions are0:
# 1. Step function
# 2. Sigmoid
# 3. Tanh
# 4. Relu
# 5. Leaky Relu
# 6. Softmax
# -------------
# 7. Softmax with logits
# 8. Softmax2d
# 9. LogSoftmax
# 10. LogSoftmax2d
# 11. SoftmaxNLLLoss
# 12. CrossEntropyLoss

# Step
# f(x) = sistema 1 if x >= 0 else 0
# not used in practice

# Sigmoid
# f(x) = 1 / (1 + e^(-x))
# output a probability between 0 and 1, used in the """last layer""" of classification

# Tanh
# f(x) = (e^(2x) - 1) / (e^(2x) + 1)
# output a value between -1 and 1, used in the """hidden layers""" of classification
# basically a scaled and shifted sigmoid function

# Relu
# f(x) = max(0, x)
# output a value between 0 and x, used in the """hidden layers""" of classification if you don't knwo what to use
# 0 for negative value and looks linear for positive values 


# Leaky Relu
# f(x) = max(0, x) + alpha * min(0, x)
# output a value between 0 and x, used in the """hidden layers""" of classification if you don't knwo what to use
# slightly improved version of relu, tries to solve the vanishing gradient problem
# also multiplies the gradient by a factor alpha
# says x if z >= 0 else a*x
# use this whenever you notice that your weights wont update during training

# Softmax
# f(x) = e^x / sum(e^x)
# output a probability between 0 and 1, used in the """last layer""" of a """multi class classification"""
